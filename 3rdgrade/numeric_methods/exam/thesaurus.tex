\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[russian]{babel}
\usepackage[left=2cm,right=1cm,top=2cm,bottom=2cm]{geometry} % Устанавливаем поля

\begin{document}
\tableofcontents
\section{Векторное пространство}
Векторное пространство $V$ над полем $\mathbb{F}$ — это множество, на котором определены две операции: сложение и умножение на скаляр, обладающие следующими свойствами:

\begin{itemize}
    \item \textbf{Ассоциативность сложения:} Для любых $u, v, w \in V$ выполнено $(u + v) + w = u + (v + w)$.
    \item \textbf{Коммутативность сложения:} Для любых $u, v \in V$ выполнено $u + v = v + u$.
    \item \textbf{Существование нулевого вектора:} Существует элемент $0 \in V$ такой, что для любого $v \in V$ выполнено $v + 0 = v$.
    \item \textbf{Существование противоположного вектора:} Для любого $v \in V$ существует элемент $-v \in V$ такой, что $v + (-v) = 0$.
    \item \textbf{Ассоциативность умножения на скаляр:} Для любых $a, b \in \mathbb{F}$ и $v \in V$ выполнено $a(bv) = (ab)v$.
    \item \textbf{Дистрибутивность относительно сложения векторов:} Для любого $a \in \mathbb{F}$ и любых $u, v \in V$ выполнено $a(u + v) = au + av$.
    \item \textbf{Дистрибутивность относительно сложения скаляров:} Для любых $a, b \in \mathbb{F}$ и $v \in V$ выполнено $(a + b)v = av + bv$.
    \item \textbf{Умножение на единицу:} Для любого $v \in V$ выполнено $1v = v$, где $1$ — это единица из поля $\mathbb{F}$.
\end{itemize}
\section{Конечномерное координатное пространство}
Конечномерное координатное пространство $\mathbb{R}^n$ — это декартово произведение $n$ копий вещественного линейного пространства $\mathbb{R}$. Формально,
\[
\mathbb{R}^n = \mathbb{R} \times \mathbb{R} \times \cdots \times \mathbb{R} \quad (n \text{ раз}).
\]
Элементы $\mathbb{R}^n$ называются векторами и записываются в виде упорядоченных $n$-мерных кортежей $(x_1, x_2, \ldots, x_n)$, где $x_i \in \mathbb{R}$ для всех $i = 1, 2, \ldots, n$. 

В этом пространстве определены операции сложения и умножения на скаляр, обладающие следующими свойствами:

\begin{itemize}
    \item \textbf{Сложение:} Для любых $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ и $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ из $\mathbb{R}^n$ их сумма определяется как
    \[
    \mathbf{u} + \mathbf{v} = (u_1 + v_1, u_2 + v_2, \ldots, u_n + v_n).
    \]
    \item \textbf{Умножение на скаляр:} Для любого $\mathbf{u} = (u_1, u_2, \ldots, u_n) \in \mathbb{R}^n$ и любого скаляра $c \in \mathbb{R}$ их произведение определяется как
    \[
    c\mathbf{u} = (cu_1, cu_2, \ldots, cu_n).
    \]
\end{itemize}
\section{Линейные отображения (линейные операторы)}
 Пусть $X, Y$ - векторные пространства над полем $\mathbb F$. Отображение $f:X\to Y$ называется линейным отображением, если $$\forall x_{1,}x_{2} \in \operatorname{dom} f,\;\lambda, \mu\in \mathbb F\quad \lambda x_{1}+ \mu x_{2}\in \operatorname{dom} f \; \text{и} \;f(\lambda x_{1}+ \mu x_{2}) = \lambda f(x_{1})+\mu f(x_{2})$$
\section{Матрицы}
Пусть есть два конечных множества:

\begin{itemize}
    \item номера строк: $M=\{1,2,\dots,m\}$;
    \item номера столбцов: $N=\{1,2,\dots,n\}$, где $m$ и $n$.
\end{itemize}

Назовём матрицей $A$ размера $m\times n$ (читается $m$ на $n$) ($m$ — '''строк''', $n$ — '''столбцов''') с элементами из некоторого кольца или поля $\mathcal{K}$ отображение вида $A\colon M\times N\to\mathcal{K}$. (прямоугольная таблица)

Матрица записывается как
$$A =
\begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n}
\\a_{21} & a_{22} & \cdots & a_{2n}
\\ \vdots & \vdots & a_{ij} & \vdots
\\ a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix},$$
где элемент матрицы $a_{ij}=a(i,j)$ находится на пересечении $i$-й ''строки'' и $j$-го ''столбца''.
\begin{itemize}
    \item $i$-я строка матрицы A(i,) =
\begin{pmatrix} a_{i1} & a_{i2} & \cdots & a_{in}
\end{pmatrix} ;
    \item $j$-й столбец матрицы A(,j) =
\begin{pmatrix}
  a_{1j} 
\\a_{2j} 
\\ \vdots 
\\ a_{mj} 
\end{pmatrix}.
\end{itemize}

При этом количество элементов матрицы равно $m \cdot n$.

В соответствии с этим

\begin{itemize}
    \item каждую строку матрицы можно интерпретировать как вектор в $n$-мерном координатном пространстве $\mathcal{K}^{n}$;
    \item каждый столбец матрицы — как вектор в $m$-мерном координатном пространстве $\mathcal{K}^{m}$.
\end{itemize}


Сама матрица естественным образом интерпретируется как вектор в пространстве $\mathcal{K}^{mn}$, имеющем размерность $mn$. Это позволяет ввести покомпонентное сложение матриц и умножение матрицы на число.

\section{Функционалы}

Пусть $V$ — векторное пространство над полем $\mathbb{F}$ (чаще всего $\mathbb{F}$ — это $\mathbb{R}$ или $\mathbb{C}$). Функционалом на $V$ называется линейное отображение $f: V \to \mathbb{F}$. Это значит, что для любых $u, v \in V$ и любого скаляра $c \in \mathbb{F}$ выполнены следующие условия:

\begin{itemize}
    \item \textbf{Линейность относительно сложения:}
    \[
    f(u + v) = f(u) + f(v).
    \]
    \item \textbf{Линейность относительно умножения на скаляр:}
    \[
    f(cu) = cf(u).
    \]
\end{itemize}
\section{Векторная норма}
Векторная норма — это функция $\|\cdot\|: V \to \mathbb{R}$, определенная на векторном пространстве $V$, которая каждому вектору $\mathbf{v} \in V$ ставит в соответствие неотрицательное действительное число $\|\mathbf{v}\|$, называемое нормой вектора $\mathbf{v}$. Векторная норма должна удовлетворять следующим условиям:

\begin{enumerate}
    \item \textbf{Неотрицательность:} Для любого $\mathbf{v} \in V$,
    \[
    \|\mathbf{v}\| \geq 0,
    \]
    причем $\|\mathbf{v}\| = 0$ тогда и только тогда, когда $\mathbf{v} = \mathbf{0}$.
    
    \item \textbf{Однородность:} Для любого $\mathbf{v} \in V$ и любого скаляра $\alpha \in \mathbb{F}$,
    \[
    \|\alpha \mathbf{v}\| = |\alpha| \cdot \|\mathbf{v}\|.
    \]
    
    \item \textbf{Неравенство треугольника:} Для любых $\mathbf{u}, \mathbf{v} \in V$,
    \[
    \|\mathbf{u} + \mathbf{v}\| \leq \|\mathbf{u}\| + \|\mathbf{v}\|.
    \]
\end{enumerate}

\subsection{Примеры векторных норм}

Рассмотрим несколько примеров векторных норм в пространстве $\mathbb{R}^n$:

\begin{itemize}
    \item \textbf{Евклидова норма (норма $L_2$):}
    \[
    \|\mathbf{v}\|_2 = \left( \sum_{i=1}^n v_i^2 \right)^{1/2}.
    \]
    
    \item \textbf{Манхэттенская норма (норма $L_1$):}
    \[
    \|\mathbf{v}\|_1 = \sum_{i=1}^n |v_i|.
    \]
    
    \item \textbf{Чебышевская норма (норма $L_\infty$):}
    \[
    \|\mathbf{v}\|_\infty = \max_{1 \leq i \leq n} |v_i|.
    \]
\end{itemize}

\subsection{Свойства векторных норм}

Некоторые важные свойства векторных норм включают:

\begin{itemize}
    \item \textbf{Эквивалентность норм:} В конечномерном векторном пространстве все нормы эквивалентны. Это значит, что если $\|\cdot\|_a$ и $\|\cdot\|_b$ — две нормы на одном и том же пространстве, то существуют положительные константы $C_1$ и $C_2$ такие, что для всех $\mathbf{v} \in V$,
    \[
    C_1 \|\mathbf{v}\|_a \leq \|\mathbf{v}\|_b \leq C_2 \|\mathbf{v}\|_a.
    \]
    
    \item \textbf{Непрерывность норм:} Все нормы на конечномерном пространстве непрерывны.
\end{itemize}
\section{Кубическая, сферическая (евклидова), октаэдрическая нормы}
Векторные нормы, которые рассматриваются в этой секции, относятся к различным видам норм, используемым для измерения длины векторов в пространстве $\mathbb{R}^n$.

\subsection{Кубическая норма (норма $L_\infty$)}

Кубическая норма, также известная как норма $L_\infty$, определяется как максимальное абсолютное значение координаты вектора. Для вектора $\mathbf{v} = (v_1, v_2, \ldots, v_n) \in \mathbb{R}^n$, кубическая норма задаётся формулой:
\[
\|\mathbf{v}\|_\infty = \max_{1 \leq i \leq n} |v_i|.
\]
Эта норма называется кубической, поскольку единичный шар в этой норме является гиперкубом.

\subsection{Сферическая норма (евклидова норма, норма $L_2$)}

Сферическая норма, также известная как евклидова норма или норма $L_2$, определяется как корень квадратный из суммы квадратов координат вектора. Для вектора $\mathbf{v} = (v_1, v_2, \ldots, v_n) \in \mathbb{R}^n$, сферическая норма задаётся формулой:
\[
\|\mathbf{v}\|_2 = \left( \sum_{i=1}^n v_i^2 \right)^{1/2}.
\]
Эта норма называется сферической, поскольку единичный шар в этой норме является гиперсферой.

\subsection{Октаэдрическая норма (норма $L_1$)}

Октаэдрическая норма, также известная как норма $L_1$, определяется как сумма абсолютных значений координат вектора. Для вектора $\mathbf{v} = (v_1, v_2, \ldots, v_n) \in \mathbb{R}^n$, октаэдрическая норма задаётся формулой:
\[
\|\mathbf{v}\|_1 = \sum_{i=1}^n |v_i|.
\]
Эта норма называется октаэдрической, поскольку единичный шар в этой норме является октаэдром в трёхмерном пространстве и аналогом октаэдра в более высоких размерностях.

\subsection{Геометрическое представление}

\begin{itemize}
    \item Единичный шар в кубической норме $L_\infty$ — гиперкуб.
    \item Единичный шар в евклидовой норме $L_2$ — гиперсфера.
    \item Единичный шар в октаэдрической норме $L_1$ — гипероктаэдр (правильный многогранник).
\end{itemize}
\section{Положительно определённая матрица}
Пусть $A$ — квадратная матрица размера $n \times n$ с действительными элементами. Матрица $A$ называется положительно определённой, если она удовлетворяет следующим условиям:

\begin{enumerate}
    \item $A$ симметрична, то есть $A = A^T$.
    \item Для любого ненулевого вектора $\mathbf{x} \in \mathbb{R}^n$ верно:
    \[
    \mathbf{x}^T A \mathbf{x} > 0.
    \]
\end{enumerate}

\subsection{Свойства положительно определённых матриц}

Положительно определённые матрицы обладают рядом важных свойств:

\begin{itemize}
    \item \textbf{Собственные значения:} Все собственные значения положительно определённой матрицы $A$ положительны.
    
    \item \textbf{Определитель:} Определитель положительно определённой матрицы $A$ положителен, то есть $\det(A) > 0$.
    
    \item \textbf{Главные миноры:} Все главные миноры положительно определённой матрицы $A$ положительны.
    
    \item \textbf{Частичная положительная определённость:} Если $A$ положительно определённая матрица, то для любого подпространства $V$ пространства $\mathbb{R}^n$ ограничение квадратичной формы $\mathbf{x}^T A \mathbf{x}$ на $V$ также положительно определено.
\end{itemize}

\subsection{Примеры положительно определённых матриц}

Рассмотрим несколько примеров положительно определённых матриц:

\begin{itemize}
    \item \textbf{Матрица Грама:} Если $A$ — матрица Грама для набора линейно независимых векторов $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \in \mathbb{R}^n$, то $A$ положительно определённая. Матрица Грама определяется как $G = V^T V$, где $V$ — матрица, составленная из столбцов $\mathbf{v}_i$.
    
    \item \textbf{Матрица ковариации:} Матрица ковариации случайного вектора $\mathbf{X} \in \mathbb{R}^n$ положительно определённая, если вектор $\mathbf{X}$ имеет полную ранг.
    
    \item \textbf{Диагональная матрица:} Диагональная матрица с положительными диагональными элементами является положительно определённой. Пусть $D = \text{diag}(d_1, d_2, \ldots, d_n)$, где $d_i > 0$ для всех $i = 1, \ldots, n$, тогда $D$ положительно определённая.
\end{itemize}

\subsection{Критерий Сильвестра}

Пусть $A$ — симметричная матрица размера $n \times n$. Для того чтобы $A$ была положительно определённой, необходимо и достаточно, чтобы все её главные миноры были положительны. Это утверждение называется критерием Сильвестра.

\section{Скалярное произведение}
Будем говорить, что в вещественном или комплексном векторном пространстве $L$ определено скалярное произведение, если каждой паре векторов $\mathbf{a}, \mathbf{b}$ из $L$ поставлено в соответствие число $(\mathbf{a}, \mathbf{b})$ из того числового поля, над которым задано $L$, удовлетворяющее следующим аксиомам.

\begin{enumerate}
    \item Для любых трёх элементов $\mathbf{a}_1, \mathbf{a}_2, \mathbf{b}$ пространства $L$ и любых чисел $\alpha, \beta$ справедливо равенство: $(\alpha \mathbf{a}_1 + \beta \mathbf{a}_2, \mathbf{b}) = \alpha (\mathbf{a}_1, \mathbf{b}) + \beta (\mathbf{a}_2, \mathbf{b})$ (линейность скалярного произведения по первому аргументу).
    \item Для любых $\mathbf{a}, \mathbf{b}$ справедливо равенство $(\mathbf{a}, \mathbf{b}) = \overline{(\mathbf{b}, \mathbf{a})}$, где черта означает комплексное сопряжение.
    \item Для любого $\mathbf{a}$ имеем: $(\mathbf{a}, \mathbf{a}) \geqslant 0$, причём $(\mathbf{a}, \mathbf{a}) = 0$ только при $\mathbf{a} = 0$ (положительная определённость и невырожденность скалярного произведения соответственно).
\end{enumerate}

Заметим, что из аксиомы 2 следует, что $(\mathbf{a}, \mathbf{a})$ — вещественное число. Поэтому аксиома 3 имеет смысл, несмотря на комплексные (в общем случае) значения скалярного произведения. Если аксиома 3 не выполняется, то произведение называется \textit{индефинитным или неопределённым}.

В $\mathbb{R}^n$ скалярное произведение двух векторов $\mathbf{a}$ и $\mathbf{b}$ определяется как:

\[
\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \dots + a_n b_n
\]
\section*{Определение нормы}

Пусть $X$ — линейное пространство над полем $\mathbb{K}$ (где $\mathbb{K}$ может быть $\mathbb{R}$ или $\mathbb{C}$).

Функция $\| \cdot \| : X \to \mathbb{R}$ называется нормой на $X$, если она удовлетворяет следующим условиям, называемым аксиомами нормы:

\begin{enumerate}
    \item \textbf{Неотрицательность:} Для любого элемента $x \in X$ выполнено $\| x \| \geq 0$, причем $\| x \| = 0$ только если $x = 0$.
    
    \item \textbf{Однородность:} Для любого элемента $x \in X$ и любого скаляра $\alpha \in \mathbb{K}$ выполнено $\| \alpha x \| = |\alpha| \cdot \| x \|$.
    
    \item \textbf{Треугольное неравенство:} Для любых элементов $x, y \in X$ выполнено $\| x + y \| \leq \| x \| + \| y \|$.
\end{enumerate}

Линейное пространство $X$, снабженное нормой, называется нормированным пространством.

Для любого вектора $x \in X$ определено скалярное произведение $(x, x)$, и норма вектора $x$ определяется следующим образом:

\[
\| x \| = \sqrt{(x, x)}
\]

где $(\cdot, \cdot)$ — скалярное произведение на $X$.

\section*{Неравенство Коши-Буняковского}

Пусть $V$ — векторное пространство над полем $\mathbb{R}$ или $\mathbb{C}$, и пусть $(\cdot, \cdot)$ — скалярное произведение на $V$. 

Тогда для любых векторов $u, v \in V$ справедливо неравенство Коши-Буняковского:

\[
| (u, v) |^2 \leq (u, u) \cdot (v, v)
\]

или, в другой форме:

\[
\left| \sum_{i=1}^{n} u_i \overline{v_i} \right|^2 \leq \left( \sum_{i=1}^{n} |u_i|^2 \right) \left( \sum_{i=1}^{n} |v_i|^2 \right)
\]

где $u = (u_1, u_2, \ldots, u_n)$ и $v = (v_1, v_2, \ldots, v_n)$ — компоненты векторов $u$ и $v$ соответственно.

\section{Энергетическая норма}
Пусть $A$ — линейный самосопряжённый и положительно определённый оператор. Тогда выражение

\[
( \mathbf{x}, \mathbf{y})_A = (A \mathbf{x}, \mathbf{y})
\]

будет удовлетворять аксиомам скалярного произведения:
\begin{enumerate}
    \item $(A \mathbf{x}, \mathbf{y}) = (\mathbf{x}, A \mathbf{y})$ (следует из самосопряжённости оператора $A$).
    \item $(A (\mathbf{x} + \mathbf{y}), \mathbf{z}) = (A \mathbf{x}, \mathbf{z}) + (A \mathbf{y}, \mathbf{z})$.
    \item $(A (\lambda \mathbf{x}), \mathbf{y}) = \lambda (A \mathbf{x}, \mathbf{y})$.
    \item $(A \mathbf{x}, \mathbf{x}) > 0$ при $\mathbf{x} \neq 0$ (следует из положительной определённости оператора $A$), $(A \mathbf{x}, \mathbf{x}) = 0$ только при $\mathbf{x} = 0$.
\end{enumerate}

Поэтому можно ввести норму $\| \mathbf{y} \| = (A \mathbf{y}, \mathbf{y})^{1/2}$, порождённую оператором $A$. Эта норма называется энергетической, а пространство с введённой нормой называется энергетическим пространством.

\section{Открытый шар, открытое множество}
Пусть $X$ — метрическое пространство с метрикой $d$. Тогда:

\subsection{Открытый шар}

Открытый шар с центром в точке $x_0 \in X$ и радиусом $r > 0$ определяется как множество:

\[
B_r(x_0) = \{ x \in X \mid d(x, x_0) < r \}
\]

\subsection{Открытое множество}

Множество $U \subseteq X$ называется открытым, если для любой точки $x \in U$ существует $\epsilon > 0$ такое, что шар $B_\epsilon(x)$ полностью содержится в $U$. То есть:

\[
\forall x \in U, \exists \epsilon > 0: B_\epsilon(x) \subseteq U
\]
\section{Пределы точки, замкнутое множество}
Пусть $X$ — метрическое пространство с метрикой $d$. Тогда:
\subsection{Предел точки}

Точка $a \in X$ называется пределом последовательности $(x_n)$, если для любого $\epsilon > 0$ существует такой номер $N$, что для всех $n \geq N$ выполняется $d(x_n, a) < \epsilon$. Обозначается это следующим образом:

\[
\lim_{n \to \infty} x_n = a
\]
\subsection{Предельная точка}

Точка $x \in X$ называется предельной точкой множества $A \subseteq X$, если в любой окрестности точки $x$ содержится хотя бы одна точка множества $A$, отличная от $x$. Формально:

\[
\forall \epsilon > 0, \exists y \in A \setminus \{ x \}: d(x, y) < \epsilon
\]
\subsection{Замкнутое множество}

Множество $F \subseteq X$ называется замкнутым, если оно содержит все свои предельные точки. То есть, если любая последовательность $(x_n) \subseteq F$, сходящаяся к точке $a \in X$, имеет предел $a \in F$. Также можно сказать, что множество $F$ замкнуто, если его дополнение $X \setminus F$ является открытым.

\section{Сфера}

Сфера с центром в точке $x_0 \in X$ и радиусом $r > 0$ определяется как множество:

\[
S_r(x_0) = \{ x \in X \mid d(x, x_0) = r \}
\]
\section{Эквивалентность векторных норм}
Две нормы $\| \cdot \|_1$ и $\| \cdot \|_2$ на линейном пространстве $X$ называются эквивалентными, если существуют положительные константы $C_1$ и $C_2$ такие, что для всех векторов $x \in X$ выполняется:

\[
C_1 \| x \|_1 \leq \| x \|_2 \leq C_2 \| x \|_1
\]

Это означает, что нормы $\| \cdot \|_1$ и $\| \cdot \|_2$ действуют примерно одинаково, с точностью до масштабирования.

\section{Матричная мультипликативная норма}
Нормой матрицы \(A\) называется действительное число \(\|A\|\), удовлетворяющее условиям:
\begin{enumerate}
    \item \(\|A\| \ge 0\), причём \(\|A\| = 0 \iff A = 0\);
    \item \(\|\alpha A\| = |\alpha| \|A\|\) для всех \(\alpha \in K\);
    \item \(\|A + B\| \le \|A\| + \|B\|\);
    \item \(\|AB\| \le \|A\| \|B\|\) (для произвольных \(n \times n\) матриц \(A\) и \(B\)).
\end{enumerate}
Иногда норму матрицы определяют только с помощью первых трёх условий (аксиом); в таком случае говорят, что определение нормы не включает свойство субмультипликативности.
\section{Подчинённая (операторная) матричная норма}
Матричная мультипликативная норма $||\cdot||$ для квадратной матрицы $A$ размера $n \times n$ определяется следующим образом:

\[
||A|| = \sup_{x \neq 0} \frac{||Ax||}{||x||} = \sup_{||x|| = 1} ||Ax||
\]

где $||\cdot||$ — норма вектора в некотором векторном пространстве, а $||Ax||$ — норма вектора, полученного умножением матрицы $A$ на вектор $x$.

\subsection{$L_1$ норма}
$L_1$ норма для матрицы, также известная как норма максимальной суммы по столбцам, определяется как:
\[
\|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}|
\]

\subsection{$L_2$ норма (спектральная норма)}
$L_2$ норма для матрицы, также известная как спектральная норма, определяется как наибольшее сингулярное число матрицы $A$:
\[
\|A\|_2 = \sigma_{\max}(A)
\]
где $\sigma_{\max}(A)$ — это наибольшее сингулярное число матрицы $A$.

\subsection{$L_\infty$ норма}
$L_\infty$ норма для матрицы, также известная как норма максимальной суммы по строкам, определяется как:
\[
\|A\|_\infty = \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}|
\]

\section{Матричная норма, согласованная с векторной}
Как и норму вектора, норму матрицы
можно вводить далеко не единственным способом. Из множества  
всевозможных норм матрицы наибольший интерес представляют такие, которые 
определенным образом соотносятся с векторными нормами, поскольку, 
чаще всего, матрицы и векторы рассматриваются в комплексе. Так, при 
умножении матрицы А на вектор х получается вектор Ах, и естественно 
потребовать, чтобы матричная норма удовлетворяла условию  
согласованности 
\[||Ax||\leq ||A|| \cdot ||x||\]
\section{Спектральный радиус}
Спектральным радиусом матрицы \(A\) называется величина \(\rho(A)\), определяемая как максимальное по модулю собственное значение этой матрицы:
\[ \rho(A) = \max \{ |\lambda| : \lambda \text{ является собственным значением матрицы } A \}. \]
\section{Лемма Келлога}

\subsection{Самосопряженный оператор}
Оператор самосопряженный, если он совпадает со своим сопряженным, т.е.
$$A, A^*: H \to H, \forall x, y \in H: \quad (Ax, y) = (x, A^* y), A^* = A$$
\subsection{Лемма Келлога}
Если положительный оператор A $((Ax, x) \geq 0 \forall x)$ является самосопряжённым в евклидовом пространстве, то 
$$||(E+A)^{-1}(E-A)||\leq 1$$
\section{Круги Гершгорина}
Круги Гершгорина — набор кругов на комплексной плоскости, определяемых по квадратной матрице, таких, что все собственные значения данной матрицы заведомо лежат внутри каких-то из этих кругов. Таким образом, они позволяют получить априорное ограничение на расположение собственных значений (локализовать спектр) квадратной матрицы

В нашем случае спектр - множество собственных чисел матрицы.

Пусть имеем комплексную матрицу. Обозначим через $R_i$ - сумма абс. значений внедиагональных элементов i-строки

Рассмотрим круг с центром в $D(a_{ii}, R_i)$ (центр, радиус)

Такой круг называется кругом Гершгорина.
\subsection{Первая теорема Гершгорина}
\textbf{Каждое с.з. матрицы лежит хотя бы в одном из кругов Гершгорина}

Аналогично, если мы строим круги гершгорина $C_j$ (по столбцам)
\subsection{Вторая теорема Гершгорина}
Если один из кругов не пересекается с другими, то он содержит только одно собственное значение.

Однако, если он пересекается с др. кругом, возможно, он не содержит с.з.

Теорема усиливается след. способом:

\textbf{если k кругов образуют связную область, изолированную от остальных n-k кругов, то первая область содержит ровно k, а вторая - n-k собств. значений матрицы 
A}

\subsection{зачем оно надо?}
Для решения матричных СЛАУ

Круги Гершгорина применяются для решения матричного уравнения вида Ax = b относительно x, где b — вектор, а A — матрица с большим числом обусловленности.

В задачах такого рода ошибка в конечном результате обычно такого же порядка величины, как и ошибка в исходных данных, умноженная на число обусловленности AНапример, если b известно с точностью до шести знаков после запятой, а число обусловленности A равно 1000, то мы можем быть уверены только в том, что x имеет точность до трех знаков после запятой. Чем больше число обусловленности, тем более неустойчив процесс решения системы.

Было бы хорошо уменьшить число обусловленности A. Это можно сделать с помощью предобуславливания. Рассматривается матрица P такая, что $P\approx A^{-1}$, и уравнение $PAx = Pb$ решается относительно x Использовать обратную к A было бы неплохо, но нахождение обратной матрицы — это то, чего мы хотим избежать из-за вычислительных затрат.

Теперь, поскольку $PA\approx I$, где I— единичная матрица, собственные значения PA будут близки к 1. По теореме Гершгорина, каждое собственное значение PA находится в пределах известной области, поэтому мы можем приблизительно оценить, насколько хорош был выбор матрицы P при помощи кругов Гершгорина.
\section{Неразложимые матрицы}
\subsection*{Определение разложимой матрицы}
Квадратная матрица $A$ порядка $n$ называется \textit{разложимой}, если существует такая перестановочная матрица $P$, что матрица $PAP^T$ имеет блочно-верхнетреугольный вид:
\[
PAP^T = \begin{pmatrix}
B & 0 \\
C & D
\end{pmatrix},
\]
где $B$ и $D$ — квадратные матрицы меньших порядков.

Если такой перестановочной матрицы $P$ не существует, то матрица $A$ называется \textit{неразложимой}.


Неразложимая матрица (или неприводимая матрица) — это квадратная матрица, которую нельзя привести к блочно-верхнетреугольному виду путем перестановки строк и столбцов.
\subsection*{Определение}
Квадратная матрица $A$ порядка $n$ называется \textit{неразложимой} или \textit{неприводимой}, если не существует такого непустого множества индексов $I \subset \{1, 2, \ldots, n\}$, что $A_{ij} = 0$ для всех $i \in I$ и $j \notin I$.

\subsection*{Свойства}
1. \textbf{Транспонирование:} Если матрица $A$ неразложима, то её транспонированная матрица $A^T$ также является неразложимой.

2. \textbf{Неотрицательные матрицы:} Если $A$ — неотрицательная матрица (все элементы матрицы $A$ неотрицательны) и $A$ неразложима, то для любой степени $k \geq n-1$ матрица $A^k$ имеет все положительные элементы.

3. \textbf{Спектральный радиус:} Спектральный радиус $\rho(A)$ неразложимой неотрицательной матрицы $A$ является простым собственным значением, соответствующим положительному собственному вектору.

4. \textbf{Блочно-верхнетреугольная форма:} Если матрица $A$ разложима, то существует такая перестановка строк и столбцов, что матрица $A$ может быть приведена к блочно-верхнетреугольному виду. Следовательно, если такой перестановки не существует, матрица $A$ является неразложимой.

\subsection*{Примеры}
\begin{itemize}
    \item Матрица
    \[
    A = \begin{pmatrix}
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0
    \end{pmatrix}
    \]
    является неразложимой.
    
    \item Матрица
    \[
    B = \begin{pmatrix}
    1 & 0 & 0 \\
    0 & 1 & 1 \\
    0 & 1 & 1
    \end{pmatrix}
    \]
    является разложимой, так как может быть приведена к блочно-верхнетреугольному виду.
\end{itemize}
\section{Неустранимая погрешность}
Неустранимая погрешность задачи связана с приближенным характером исходной содержательной модели (в частности, с невозможностью учесть все факторы в процессе изучения моделируемого явления), а также её математического описания, параметры которого обычно являются приближенными числами (например, из-за принципиальной невозможности выполнения абсолютно точных измерений). Для вычислителя погрешность задачи следует считать неустранимой (безусловной), хотя постановщик задачи иногда может её изменить.

Основные аспекты неустранимой погрешности:
\begin{itemize}
    \item \textbf{Неисчерпывающая модель:} Модель может не учитывать все факторы, влияющие на явление, из-за чего возникают дополнительные погрешности.
    \item \textbf{Неточность параметров:} Параметры математической модели часто являются приближенными числами, что приводит к дополнительным ошибкам.
\end{itemize}
\section{Погрешность метода решения}
Погрешность метода решения связана с приближениями, которые вносятся при выборе и реализации численного метода для решения задачи. Эта погрешность возникает из-за аппроксимации, используемой в методе, и может включать такие аспекты, как:
\begin{itemize}
    \item \textbf{Дискретизация:} Преобразование непрерывной задачи в дискретную форму может вносить ошибки.
    \item \textbf{Схема аппроксимации:} Выбор аппроксимационных схем может существенно влиять на точность решения.
\end{itemize}

\section{Вычислительная погрешность}
Вычислительная погрешность возникает в процессе выполнения вычислений на компьютере и связана с ограниченной точностью представления чисел и операций с ними. Основные источники вычислительной погрешности включают:
\begin{itemize}
    \item \textbf{Ограниченная точность представления:} Числа в компьютере хранятся с ограниченной точностью, что приводит к округлению и накоплению ошибок.
    \item \textbf{Ошибки округления:} При выполнении арифметических операций происходит округление чисел, что вносит дополнительную погрешность.
    \item \textbf{Ошибки вычислений:} Накопление мелких ошибок в результате многочисленных операций может привести к значительным отклонениям в конечном результате.
\end{itemize}
\section{Устойчивая конечноразмерная модель}
Устойчивая конечноразмерная модель характеризуется тем, что небольшие изменения в начальных данных или параметрах модели приводят к небольшим изменениям в решении. Это свойство устойчивости позволяет гарантировать надёжность и предсказуемость результатов моделирования. Основные аспекты устойчивой конечноразмерной модели:
\begin{itemize}
    \item \textbf{Чувствительность к начальным условиям:} Модель не должна значительно изменяться при небольших изменениях начальных условий.
    \item \textbf{Стабильность численного метода:} Выбранный численный метод должен сохранять устойчивость в процессе вычислений.
\end{itemize}

\section{Прямой анализ ошибки}
Прямой анализ ошибки включает изучение того, как ошибки исходных данных и приближений влияют на конечный результат вычислений. Целью прямого анализа является оценка точности решения и понимание того, какие компоненты модели или вычислительного процесса вносят наибольший вклад в общую ошибку. Важные аспекты прямого анализа ошибки:
\begin{itemize}
    \item \textbf{Оценка ошибки аппроксимации:} Анализ того, насколько хорошо численный метод приближает истинное решение.
    \item \textbf{Анализ чувствительности:} Оценка влияния изменений входных данных на результат.
\end{itemize}
\textbf{Прямая ошибка алгоритма} — это разница между результатом и решением. В этом случае 
$$\Delta y = y^* - y$$
\section{Обратный анализ ошибки}
Обратный анализ ошибки направлен на определение допустимых изменений исходных данных или параметров модели, которые приводят к приемлемым изменениям в решении. Этот анализ позволяет оценить надежность модели и определить критические параметры, которые требуют точного измерения или особого контроля. Основные аспекты обратного анализа ошибки:
\begin{itemize}
    \item \textbf{Допустимые пределы изменений:} Определение диапазонов изменений входных данных, при которых решение остаётся приемлемым.
    \item \textbf{Анализ устойчивости модели:} Исследование того, насколько изменения параметров модели влияют на её устойчивость и точность.
\end{itemize}
\textbf{Обратная ошибка} — это наименьшая $\Delta x$ такая, что $$f(x + \Delta x) = y^*.$$ Другими словами, обратная ошибка говорит, какую проблему на самом деле решил алгоритм.
\section{Обусловленность}
Обусловленность численного решения задачи измеряет чувствительность решения к малым изменениям исходных данных. В частности, обусловленность матрицы \(A\) по отношению к решению линейной системы уравнений \(Ax = b\) определяется как:

\[ \nu(A) = \|A\| \|A^{-1}\| \]

где \(\|\cdot\|\) обозначает выбранную норму (например, спектральную норму или норму Фробениуса).

Высокое значение \(\nu(A)\) указывает на плохо обусловленную матрицу, что означает, что малые изменения в векторе \(b\) могут приводить к большим изменениям в решении \(x\). Низкое значение \(\nu(A)\), напротив, указывает на хорошо обусловленную матрицу, где решение менее чувствительно к возмущениям в данных.

$$\frac{|\lambda_{max}(A)|}{|\lambda_{min}(A)|}\leqslant \nu (A)$$
\section{Прямые методы решения задач линейной алгебры}
Такие методы, которые приводят к решению 
за конечное число арифметических операций. Если операции реализуются 
точно, то и решение также будет точным (в связи с чем к классу прямых 
методов применяют еще название точные методы). 
\subsection{Метод Гаусса}
Метод Гаусса используется для решения систем линейных уравнений вида $A\mathbf{x} = \mathbf{b}$, где $A$ — квадратная матрица, $\mathbf{x}$ — вектор неизвестных, а $\mathbf{b}$ — вектор правых частей. Основные этапы метода включают:
\begin{itemize}
    \item \textbf{Прямой ход:} Преобразование матрицы $A$ к верхнетреугольному виду с помощью элементарных преобразований строк.
    \item \textbf{Обратный ход:} Решение полученной системы уравнений методом обратной подстановки.
\end{itemize}

\subsection{Метод Гаусса-Жордана}
Метод Гаусса-Жордана является расширением метода Гаусса и включает приведение матрицы $A$ к диагональному или единичному виду. Этот метод полезен для нахождения обратной матрицы и решения систем линейных уравнений.

\subsection{LU-разложение}
LU-разложение представляет матрицу $A$ в виде произведения нижнетреугольной матрицы $L$ и верхнетреугольной матрицы $U$: $A = LU$. После разложения система $A\mathbf{x} = \mathbf{b}$ решается в два этапа:
\begin{itemize}
    \item \textbf{Решение системы $L\mathbf{y} = \mathbf{b}$} методом прямой подстановки.
    \item \textbf{Решение системы $U\mathbf{x} = \mathbf{y}$} методом обратной подстановки.
\end{itemize}

\subsection{QR-разложение}
QR-разложение представляет матрицу $A$ в виде произведения ортогональной матрицы $Q$ и верхнетреугольной матрицы $R$: $A = QR$. Этот метод используется для решения систем линейных уравнений и нахождения собственных значений матрицы.

\subsection{Метод Холецкого}
Метод Холецкого используется для разложения положительно определённых матриц на произведение нижнетреугольной матрицы и её транспонированной: $A = LL^T$. Этот метод эффективен для решения симметричных и положительно определённых систем линейных уравнений.

\subsection{Метод прогонки}
Метод прогонки применяется для решения трёхдиагональных систем линейных уравнений вида $A\mathbf{x} = \mathbf{b}$, где $A$ — трёхдиагональная матрица. Метод прогонки включает следующие этапы:
\begin{itemize}
    \item \textbf{Прямой ход:} Преобразование исходной системы к верхнетреугольному виду.
    \item \textbf{Обратный ход:} Решение системы уравнений методом обратной подстановки.
\end{itemize}

\subsection{Метод вращений}
Метод вращений (метод Якоби) используется для нахождения собственных значений и собственных векторов симметричных матриц. Основная идея метода заключается в последовательном применении ортогональных преобразований для приведения матрицы к диагональному виду. Этапы метода вращений включают:
\begin{itemize}
    \item \textbf{Преобразование матрицы:} Поворот на заданный угол для обнуления внедиагональных элементов.
    \item \textbf{Итерация:} Повторение процесса до тех пор, пока матрица не станет диагональной.
\end{itemize}

\begin{enumerate}
    \item Находят точное решение (с учетом ограничений машинной арифметики) за конечное число шагов.
    \item Требуют больше памяти, так как хранят полную матрицу системы.
    \item Обычно сходятся быстрее, но менее эффективны для больших систем.
\end{enumerate}
\section{Ортогональные элементарные преобразования}
Ортогональные элементарные преобразования используются для приведения матриц к более простому виду и обладают свойством сохранения длины векторов и углов между ними. Эти преобразования включают элементарные операции, такие как вращение и отражение, и выполняются с помощью ортогональных матриц, то есть таких матриц $Q$, для которых выполняется условие $Q^T Q = I$, где $Q^T$ — транспонированная матрица $Q$, а $I$ — единичная матрица.

\section{Элементарное преобразование вращения}
Элементарное преобразование вращения (или преобразование Гивенса) используется для зануления отдельных элементов матрицы путём вращения плоскости, образованной двумя координатными осями. Для двумерного случая матрица вращения $G(i, j, \theta)$ имеет вид:
\[
G(i, j, \theta) = \begin{pmatrix}
1 & & & & & \\
& \ddots & & & & \\
& & \cos \theta & & -\sin \theta & \\
& & & 1 & & \\
& & \sin \theta & & \cos \theta & \\
& & & & & \ddots & \\
\end{pmatrix},
\]
где угол $\theta$ выбран таким образом, чтобы обнулить элемент $(i, j)$ матрицы. Применение матрицы $G(i, j, \theta)$ к матрице $A$ можно записать как $G^T A G$.

\section{Элементарное преобразование отражения}
Элементарное преобразование отражения (или преобразование Хаусхолдера) используется для зануления всех элементов столбца матрицы, кроме одного. Матрица отражения $H$ для вектора $\mathbf{v}$ определяется как:
\[
H = I - 2 \frac{\mathbf{v} \mathbf{v}^T}{\mathbf{v}^T \mathbf{v}},
\]
где $I$ — единичная матрица. Преобразование Хаусхолдера отражает вектор относительно гиперплоскости, ортогональной $\mathbf{v}$, и применяется к матрице $A$ для упрощения её структуры.

ИЛИ

$$H =  E - 2 W W^T, (W, W) = 1$$

\section{Методы Гаусса, Жордана, LU-разложение (треугольная факторизация)}
Методы Гаусса, Гаусса-Жордана и LU-разложение (факторизация на нижнетреугольную и верхнетреугольную матрицы) являются прямыми методами решения систем линейных уравнений. Они основываются на последовательном применении элементарных преобразований строк матрицы системы, с целью приведения её к более простому виду и последующего нахождения решения.

\subsection{Gauss}
Метод Гаусса используется для решения систем линейных уравнений вида \(A \mathbf{x} = \mathbf{b}\), где \(A\) — матрица коэффициентов, \(\mathbf{x}\) — вектор неизвестных, а \(\mathbf{b}\) — вектор правых частей.

Алгоритм состоит из следующих шагов:
\begin{enumerate}
    \item Прямой ход:
    \begin{enumerate}
        \item Для каждой строки \(i\) от 1 до \(n-1\):
        \begin{enumerate}
            \item Найти ведущий элемент в строке \(i\) и, если он равен нулю, поменять строку \(i\) с одной из нижних строк, где ведущий элемент не равен нулю.
            \item Для всех строк \(j > i\):
            \begin{itemize}
                \item Вычислить множитель \(m_{ji} = \frac{a_{ji}}{a_{ii}}\).
                \item Обновить строку \(j\): \(\mathbf{a}_j = \mathbf{a}_j - m_{ji} \mathbf{a}_i\).
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
    \item Обратный ход:
    \begin{enumerate}
        \item Для каждой строки \(i\) от \(n\) до 1:
        \begin{itemize}
            \item Найти решение для \(x_i\): \(x_i = \frac{b_i - \sum_{j=i+1}^{n} a_{ij} x_j}{a_{ii}}\).
        \end{itemize}
    \end{enumerate}
\end{enumerate}

\subsection{Jordan}

Метод Гаусса-Жордана используется для решения систем линейных уравнений и приведения матрицы к диагональному виду.

Алгоритм состоит из следующих шагов:
\begin{enumerate}
    \item Прямой ход:
    \begin{enumerate}
        \item Для каждой строки \(i\) от 1 до \(n\):
        \begin{enumerate}
            \item Найти ведущий элемент в строке \(i\) и, если он равен нулю, поменять строку \(i\) с одной из нижних строк, где ведущий элемент не равен нулю.
            \item Нормализовать строку \(i\), разделив ее на ведущий элемент.
            \item Для всех строк \(j \neq i\):
            \begin{itemize}
                \item Вычислить множитель \(m_{ji} = a_{ji}\).
                \item Обновить строку \(j\): \(\mathbf{a}_j = \mathbf{a}_j - m_{ji} \mathbf{a}_i\).
            \end{itemize}
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
\subsection{LU}
LU-разложение матрицы \(A\) представляет собой факторизацию вида \(A = LU\), где \(L\) — нижнетреугольная матрица, а \(U\) — верхнетреугольная матрица. После разложения матрицы решаем систему линейных уравнений \(A\mathbf{x} = \mathbf{b}\).

Алгоритм состоит из следующих шагов:
\begin{enumerate}
    \item Инициализация матриц \(L\) и \(U\):
    \begin{align*}
        L &= \begin{pmatrix}
        1 & 0 & 0 & \cdots & 0 \\
        l_{21} & 1 & 0 & \cdots & 0 \\
        l_{31} & l_{32} & 1 & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        l_{n1} & l_{n2} & l_{n3} & \cdots & 1 \\
        \end{pmatrix}, \\
        U &= \begin{pmatrix}
        u_{11} & u_{12} & u_{13} & \cdots & u_{1n} \\
        0 & u_{22} & u_{23} & \cdots & u_{2n} \\
        0 & 0 & u_{33} & \cdots & u_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & 0 & \cdots & u_{nn} \\
        \end{pmatrix}.
    \end{align*}
    \item Для каждой строки \(i\) от 1 до \(n\):
    \begin{enumerate}
        \item Для каждого столбца \(j\) от \(i\) до \(n\):
        \begin{itemize}
            \item Вычислить \(u_{ij} = a_{ij} - \sum_{k=1}^{i-1} l_{ik} u_{kj}\).
        \end{itemize}
        \item Для каждого столбца \(j\) от \(i+1\) до \(n\):
        \begin{itemize}
            \item Вычислить \(l_{ji} = \frac{a_{ji} - \sum_{k=1}^{i-1} l_{jk} u_{ki}}{u_{ii}}\).
        \end{itemize}
    \end{enumerate}
    \item Решение системы уравнений \(A\mathbf{x} = \mathbf{b}\):
    \begin{enumerate}
        \item Решение системы \(L\mathbf{y} = \mathbf{b}\) (прямой ход):
        \begin{itemize}
            \item Для \(i\) от 1 до \(n\): \(y_i = b_i - \sum_{j=1}^{i-1} l_{ij} y_j\).
        \end{itemize}
        \item Решение системы \(U\mathbf{x} = \mathbf{y}\) (обратный ход):
        \begin{itemize}
            \item Для \(i\) от \(n\) до 1: \(x_i = \frac{y_i - \sum_{j=i+1}^{n} u_{ij} x_j}{u_{ii}}\).
        \end{itemize}
    \end{enumerate}
\end{enumerate}
\section{Метод прогонки}
Метод прогонки применяется для эффективного решения трёхдиагональных систем линейных уравнений. Этот метод включает прямой ход, в ходе которого вычисляются прогоночные коэффициенты, и обратный ход, при котором находится решение системы.

Метод прогонки используется для решения системы линейных уравнений вида:
\[
\begin{cases}
a_1 x_1 + b_1 x_2 = d_1 \\
c_2 x_1 + a_2 x_2 + b_2 x_3 = d_2 \\
\vdots \\
c_i x_{i-1} + a_i x_i + b_i x_{i+1} = d_i \\
\vdots \\
c_n x_{n-1} + a_n x_n = d_n
\end{cases}
\]
где \(a_i\), \(b_i\), \(c_i\) и \(d_i\) — элементы лент матрицы.

Алгоритм метода прогонки состоит из следующих шагов:

\begin{enumerate}
    \item Прямой ход:
    \begin{enumerate}
        \item Вычислить коэффициенты \(P\) и \(Q\):
        \begin{align*}
            P_1 &= \frac{b_1}{a_1}, \quad Q_1 = \frac{d_1}{a_1}, \\
            P_i &= \frac{b_i}{a_i - c_i P_{i-1}}, \quad Q_i = \frac{d_i - c_i Q_{i-1}}{a_i - c_i P_{i-1}}, \quad \text{для} \ i = 2, \ldots, n-1, \\
            P_n &= 0, \quad Q_n = \frac{d_n - c_n Q_{n-1}}{a_n - c_n P_{n-1}}.
        \end{align*}
    \end{enumerate}
    \item Обратный ход:
    \begin{enumerate}
        \item Найти решение \(x_i\):
        \begin{align*}
            x_n &= Q_n, \\
            x_i &= P_i x_{i+1} + Q_i, \quad \text{для} \ i = n-1, \ldots, 1.
        \end{align*}
    \end{enumerate}
\end{enumerate}
\section{Стационарные итерационные методы}
Стационарные итерационные методы представляют собой класс численных методов решения систем линейных уравнений, в которых итерационный процесс используется для приближенного нахождения решения. К таким методам относятся метод простой итерации (Якоби), метод Зейделя и метод  релаксации.

\section{Начальное приближение}
Начальное приближение в итерационных методах является начальным значением вектора, с которого начинается итерационный процесс. Качество начального приближения может существенно влиять на скорость сходимости метода.

\section{Шаг итерационного процесса}
Шаг итерационного процесса представляет собой одну итерацию алгоритма, в ходе которой текущее приближение решения обновляется в соответствии с выбранным оператором перехода.

\section{Оператор перехода (оператор шага) итерационного процесса}
Оператор перехода в итерационном процессе определяет правило, по которому происходит обновление текущего приближения к решению системы линейных уравнений на каждом шаге итерации. Он зависит от выбранного итерационного метода.

\section{Метод простой итерации}
Метод простой итерации (или метод Якоби) является одним из стационарных итерационных методов для решения систем линейных уравнений. Он основан на разложении матрицы системы на диагональную и недиагональную части, и применяет итерационную формулу для обновления приближения решения.

\section{Стационарная точка}
Стационарная точка функции — это точка, в которой градиент функции равен нулю. В контексте итерационных методов решения линейных уравнений, стационарная точка соответствует точке, где метод сходится к решению. Для системы линейных уравнений $A\mathbf{x} = \mathbf{b}$ стационарная точка $\mathbf{x}^*$ удовлетворяет условию:
\[
A\mathbf{x}^* = \mathbf{b}.
\]

\section{Метод Зейделя "ускорения" метода простой итерации}
Метод Зейделя представляет собой модификацию метода простой итерации, которая позволяет ускорить сходимость. В отличие от метода Якоби, где обновления всех компонент вектора решений происходят одновременно, в методе Зейделя каждое новое значение компоненты используется немедленно в последующих вычислениях. Формула метода Зейделя:
\[
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right).
\]

\section{Асимптотическая скорость сходимости}
Асимптотическая скорость сходимости описывает скорость, с которой последовательность итераций приближается к точному решению. Если $e^{(k)}$ обозначает ошибку на $k$-й итерации, то асимптотическая скорость сходимости определяется как:
\[
\lim_{k \to \infty} \frac{\|e^{(k+1)}\|}{\|e^{(k)}\|}.
\]

\section{Одношаговый метод Ричардсона}
Одношаговый метод Ричардсона применяется для решения систем линейных уравнений и основан на итерационной схеме:
\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha (\mathbf{b} - A\mathbf{x}^{(k)}),
\]
где $\alpha$ — параметр метода.

\section{Оптимальное значение параметра одношагового метода Ричардсона}
Оптимальное значение параметра $\alpha$ в одношаговом методе Ричардсона зависит от спектральных свойств матрицы $A$. Для симметричной положительно определённой матрицы $A$ оптимальное значение $\alpha$ определяется как:
\[
\alpha_{\text{opt}} = \frac{2}{\lambda_{\min} + \lambda_{\max}},
\]
где $\lambda_{\min}$ и $\lambda_{\max}$ — минимальное и максимальное собственные значения матрицы $A$ соответственно.

\section{Метод Якоби}
Метод Якоби является одним из стационарных итерационных методов для решения систем линейных уравнений вида \(A\mathbf{x} = \mathbf{b}\), где \(A\) — матрица коэффициентов, \(\mathbf{x}\) — вектор неизвестных, а \(\mathbf{b}\) — вектор правых частей.

\subsection*{Основная идея метода}

Метод Якоби основан на разложении матрицы \(A\) на диагональную часть \(D\) и недиагональную часть \(R\), где \(A = D + R\). Тогда система уравнений \(A\mathbf{x} = \mathbf{b}\) может быть переписана в виде:
\[
D\mathbf{x} = \mathbf{b} - R\mathbf{x}.
\]

Итерационная формула метода Якоби для обновления приближения решения \(\mathbf{x}^{(k)}\) на \(k\)-й итерации:
\[
\mathbf{x}^{(k+1)} = D^{-1} (\mathbf{b} - R\mathbf{x}^{(k)}).
\]

\subsection*{Алгоритм метода Якоби}

Алгоритм метода Якоби включает следующие шаги:
\begin{enumerate}
    \item Задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Для каждого \(i\)-го уравнения системы:
        \[
        x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{\substack{j=1 \\ j \neq i}}^n a_{ij} x_j^{(k)} \right),
        \]
        где \(a_{ii}\) — диагональные элементы матрицы \(A\), \(a_{ij}\) — недиагональные элементы матрицы \(A\), \(b_i\) — элементы вектора \(\mathbf{b}\).
    \end{enumerate}
\end{enumerate}

\subsection*{Преимущества и недостатки}

Метод Якоби обладает следующими преимуществами и недостатками:

\begin{itemize}
    \item \textbf{Преимущества}:
    \begin{itemize}
        \item Простота реализации.
        \item Возможность параллельной обработки, так как обновление каждого компонента \(\mathbf{x}\) зависит только от предыдущих итераций.
    \end{itemize}
    \item \textbf{Недостатки}:
    \begin{itemize}
        \item Медленная сходимость для некоторых систем уравнений.
        \item Требование диагонального преобладания матрицы \(A\) для гарантии сходимости.
    \end{itemize}
\end{itemize}

\subsection*{Условие сходимости}

Метод Якоби сходится, если матрица \(A\) является диагонально доминирующей, то есть выполняется условие:
\[
|a_{ii}| > \sum_{\substack{j=1 \\ j \neq i}}^n |a_{ij}| \quad \text{для всех} \ i.
\]

\section{Метод Гаусса-Зейделя}
Метод Гаусса-Зейделя представляет собой улучшение метода Якоби, при котором обновлённые значения компонент используются сразу же в следующих вычислениях:
\[
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right).
\]
Метод Гаусса-Зейделя является стационарным итерационным методом для решения систем линейных уравнений вида \(A\mathbf{x} = \mathbf{b}\), где \(A\) — матрица коэффициентов, \(\mathbf{x}\) — вектор неизвестных, а \(\mathbf{b}\) — вектор правых частей.

\subsection*{Основная идея метода}

Метод Гаусса-Зейделя является модификацией метода Якоби, в котором обновленные значения используются сразу же в текущей итерации. Матрица \(A\) разлагается на нижнюю треугольную часть \(L\), диагональную часть \(D\) и верхнюю треугольную часть \(U\):
\[
A = L + D + U.
\]
Итерационная формула метода Гаусса-Зейделя:
\[
\mathbf{x}^{(k+1)} = (D + L)^{-1} (\mathbf{b} - U \mathbf{x}^{(k)}).
\]

\subsection*{Алгоритм метода Гаусса-Зейделя}

Алгоритм метода Гаусса-Зейделя включает следующие шаги:
\begin{enumerate}
    \item Задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Для каждого \(i\)-го уравнения системы:
        \[
        x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right),
        \]
        где \(a_{ii}\) — диагональные элементы матрицы \(A\), \(a_{ij}\) — недиагональные элементы матрицы \(A\), \(b_i\) — элементы вектора \(\mathbf{b}\).
    \end{enumerate}
\end{enumerate}

\subsection*{Преимущества и недостатки}

Метод Гаусса-Зейделя обладает следующими преимуществами и недостатками:

\begin{itemize}
    \item \textbf{Преимущества}:
    \begin{itemize}
        \item Быстрая сходимость по сравнению с методом Якоби.
        \item Меньшее количество итераций для достижения заданной точности.
    \end{itemize}
    \item \textbf{Недостатки}:
    \begin{itemize}
        \item Не всегда возможна параллельная обработка, так как обновленные значения используются сразу же.
        \item Требование диагонального преобладания матрицы \(A\) для гарантии сходимости.
    \end{itemize}
\end{itemize}

\subsection*{Условие сходимости}

Метод Гаусса-Зейделя сходится, если матрица \(A\) является диагонально доминирующей, то есть выполняется условие:
\[
|a_{ii}| > \sum_{j \neq i} |a_{ij}| \quad \text{для всех} \ i.
\]
\section{Методы релаксации}
Методы релаксации являются итерационными методами решения систем линейных уравнений, которые включают введение релаксационного параметра $\omega$ для улучшения сходимости. Примеры методов релаксации включают метод сверхрелаксации (SOR).
Методы релаксации являются итерационными методами для решения систем линейных уравнений \(A\mathbf{x} = \mathbf{b}\), где \(A\) — матрица коэффициентов, \(\mathbf{x}\) — вектор неизвестных, а \(\mathbf{b}\) — вектор правых частей. Эти методы включают введение релаксационного параметра \(\omega\) для улучшения сходимости. Одним из таких методов является метод сверхрелаксации (SOR).

\subsection*{Метод сверхрелаксации (SOR)}

Метод сверхрелаксации (SOR) является улучшением метода Гаусса-Зейделя и включает дополнительный параметр \(\omega\) (релаксационный параметр), который ускоряет сходимость метода. Алгоритм SOR состоит из следующих шагов:

\begin{enumerate}
    \item Начальная инициализация: задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Для \(i = 1, 2, \ldots, n\):
        \begin{itemize}
            \item Вычислить:
            \[
            x_i^{(k+1)} = (1 - \omega) x_i^{(k)} + \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right)
            \]
        \end{itemize}
    \end{enumerate}
\end{enumerate}

Здесь \(x_i^{(k+1)}\) — новое значение компоненты \(x_i\) на \(k+1\)-й итерации, \(x_i^{(k)}\) — значение компоненты \(x_i\) на \(k\)-й итерации, \(a_{ij}\) — элементы матрицы \(A\), \(b_i\) — компоненты вектора \(\mathbf{b}\), и \(\omega\) — релаксационный параметр (обычно \(1 < \omega < 2\)).

\subsection*{Выбор релаксационного параметра}

Выбор оптимального релаксационного параметра \(\omega\) зависит от специфики задачи и структуры матрицы \(A\). Значение \(\omega = 1\) соответствует методу Гаусса-Зейделя. Для ускорения сходимости обычно выбирают \(1 < \omega < 2\). В некоторых случаях оптимальное значение \(\omega\) можно определить экспериментально.

\section{Методы расщепления}
Методы расщепления основаны на разбиении матрицы $A$ на сумму матриц $M$ и $N$ таким образом, что решение системы $A\mathbf{x} = \mathbf{b}$ эквивалентно решению итерационной схемы:
\[
M \mathbf{x}^{(k+1)} = \mathbf{b} - N \mathbf{x}^{(k)}.
\]
Примеры методов расщепления включают методы Якоби и Гаусса-Зейделя.

\section{Нестационарные итерационные методы}
Нестационарные итерационные методы используют параметры, которые могут изменяться на каждой итерации. Эти методы включают, например, метод сопряжённых градиентов и метод минимальных невязок.

\section{Методы вариационного типа}
Методы вариационного типа включают в себя подходы, основанные на минимизации функционалов, связанных с системой линейных уравнений. Примером такого метода является метод минимальных невязок.

\section{Метод минимальных невязок}
Метод минимальных невязок (МНН) направлен на минимизацию нормы невязки на каждой итерации. Итерационный процесс определяется как:
\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)},
\]
где $\mathbf{r}^{(k)} = \mathbf{b} - A\mathbf{x}^{(k)}$ — невязка на $k$-й итерации, а $\alpha_k$ выбирается для минимизации нормы невязки.

Метод минимальных невязок (MNR) используется для решения систем линейных уравнений \(A\mathbf{x} = \mathbf{b}\), где \(A\) — матрица коэффициентов, \(\mathbf{x}\) — вектор неизвестных, \(\mathbf{b}\) — вектор правых частей.

\subsection*{Основная идея метода}

Основная идея метода заключается в минимизации нормы вектора невязки \(\mathbf{r}^{(k)} = \mathbf{b} - A\mathbf{x}^{(k)}\) на каждой итерации. Итерационная формула метода минимальных невязок:
\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)},
\]
где \(\alpha_k\) — параметр, минимизирующий норму вектора невязки.

\subsection*{Алгоритм метода минимальных невязок}

Алгоритм метода минимальных невязок включает следующие шаги:
\begin{enumerate}
    \item Задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Вычислить начальную невязку \(\mathbf{r}^{(0)} = \mathbf{b} - A\mathbf{x}^{(0)}\).
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Вычислить параметр \(\alpha_k\):
        \[
        \alpha_k = \frac{\mathbf{r}^{(k)T} \mathbf{r}^{(k)}}{\mathbf{r}^{(k)T} A \mathbf{r}^{(k)}}.
        \]
        \item Обновить приближение:
        \[
        \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{r}^{(k)}.
        \]
        \item Вычислить новую невязку:
        \[
        \mathbf{r}^{(k+1)} = \mathbf{r}^{(k)} - \alpha_k A \mathbf{r}^{(k)}.
        \]
    \end{enumerate}
\end{enumerate}
\section{Метод минимальных ошибок}
Метод минимальных ошибок (ММО) фокусируется на минимизации ошибки на каждой итерации. Это достигается выбором оптимального шага $\alpha_k$ на каждой итерации для минимизации нормы ошибки.

\subsection*{Основная идея метода}

Основная идея метода заключается в минимизации нормы ошибки \(\mathbf{e}^{(k)} = \mathbf{x}^{*} - \mathbf{x}^{(k)}\), где \(\mathbf{x}^{*}\) — точное решение, а \(\mathbf{x}^{(k)}\) — приближение на \(k\)-й итерации.

\subsection*{Алгоритм метода минимальных ошибок}

Алгоритм метода минимальных ошибок включает следующие шаги:
\begin{enumerate}
    \item Задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Вычислить начальную ошибку \(\mathbf{e}^{(0)} = \mathbf{x}^{*} - \mathbf{x}^{(0)}\).
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Вычислить параметр \(\beta_k\):
        \[
        \beta_k = \frac{\mathbf{e}^{(k)T} A \mathbf{e}^{(k)}}{\mathbf{e}^{(k)T} A^2 \mathbf{e}^{(k)}}.
        \]
        \item Обновить приближение:
        \[
        \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \beta_k \mathbf{e}^{(k)}.
        \]
        \item Вычислить новую ошибку:
        \[
        \mathbf{e}^{(k+1)} = \mathbf{e}^{(k)} - \beta_k A \mathbf{e}^{(k)}.
        \]
    \end{enumerate}
\end{enumerate}
\section{Полиномы Чебышева}
Полиномы Чебышева $T_n(x)$ являются специальным классом ортогональных полиномов, которые имеют важные свойства минимизации отклонений. Они удовлетворяют рекуррентному соотношению:
\[
T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x),
\]
с начальными условиями $T_0(x) = 1$ и $T_1(x) = x$.

Полиномы Чебышёва являются важным классом ортогональных полиномов, используемых в численном анализе и аппроксимации функций. Существует два типа полиномов Чебышёва: полиномы Чебышёва первого рода \( T_n(x) \) и полиномы Чебышёва второго рода \( U_n(x) \).

\subsection*{Полиномы Чебышёва первого рода}

Полиномы Чебышёва первого рода \( T_n(x) \) определяются рекурсивным соотношением:
\begin{align*}
T_0(x) &= 1, \\
T_1(x) &= x, \\
T_{n+1}(x) &= 2x T_n(x) - T_{n-1}(x) \quad \text{для} \ n \geq 1.
\end{align*}

Явная форма полиномов Чебышёва первого рода:
\[
T_n(x) = \cos(n \arccos(x)).
\]

\subsection*{Полиномы Чебышёва второго рода}

Полиномы Чебышёва второго рода \( U_n(x) \) определяются рекурсивным соотношением:
\begin{align*}
U_0(x) &= 1, \\
U_1(x) &= 2x, \\
U_{n+1}(x) &= 2x U_n(x) - U_{n-1}(x) \quad \text{для} \ n \geq 1.
\end{align*}

Явная форма полиномов Чебышёва второго рода:
\[
U_n(x) = \frac{\sin((n+1) \arccos(x))}{\sin(\arccos(x))}.
\]

\subsection*{Свойства полиномов Чебышёва}

Полиномы Чебышёва обладают следующими важными свойствами:

\begin{enumerate}
    \item \textbf{Ортогональность}:
    Полиномы Чебышёва первого рода \( T_n(x) \) ортогональны на интервале \([-1, 1]\) с весовой функцией \((1 - x^2)^{-1/2}\):
    \[
    \int_{-1}^{1} \frac{T_n(x) T_m(x)}{\sqrt{1 - x^2}} \, dx = 
    \begin{cases} 
    0 & \text{если} \ n \neq m, \\
    \pi & \text{если} \ n = m = 0, \\
    \frac{\pi}{2} & \text{если} \ n = m \neq 0.
    \end{cases}
    \]

    Полиномы Чебышёва второго рода \( U_n(x) \) ортогональны на интервале \([-1, 1]\) с весовой функцией \((1 - x^2)^{1/2}\):
    \[
    \int_{-1}^{1} U_n(x) U_m(x) \sqrt{1 - x^2} \, dx = 
    \begin{cases} 
    0 & \text{если} \ n \neq m, \\
    \frac{\pi}{2} & \text{если} \ n = m.
    \end{cases}
    \]

    \item \textbf{Экстремальные значения}:
    Полином \( T_n(x) \) принимает значения \(-1\) и \(1\) в \(n+1\) точке на интервале \([-1, 1]\).

    \item \textbf{Минимизация отклонения}:
    Полиномы \( T_n(x) \) минимизируют максимальное отклонение от нуля среди всех полиномов степени \( n \) с ведущим коэффициентом \( 2^{n-1} \).

    \item \textbf{Корни полиномов Чебышёва}:
    Корни полиномов \( T_n(x) \) (узлы Чебышёва) равны:
    \[
    x_k = \cos\left(\frac{2k-1}{2n}\pi\right), \quad k = 1, 2, \ldots, n.
    \]
    Корни полиномов \( U_n(x) \) равны:
    \[
    x_k = \cos\left(\frac{k}{n+1}\pi\right), \quad k = 1, 2, \ldots, n.
    \]

    \item \textbf{Симметрия}:
    Полиномы \( T_n(x) \) и \( U_n(x) \) обладают чётной или нечётной симметрией:
    \[
    T_n(-x) = (-1)^n T_n(x), \quad U_n(-x) = (-1)^n U_n(x).
    \]
\end{enumerate}

\section{Полином, наименее отклоняющийся от нуля на отрезке}
Полином, наименее отклоняющийся от нуля на отрезке, может быть найден с использованием полиномов Чебышева. Для отрезка $[-1, 1]$ таким полиномом является $T_n(x)$, который минимизирует максимальное отклонение от нуля.

\section{Метод Ричардсона с чебышевскими параметрами}
Метод Ричардсона с Чебышевскими параметрами является итерационным методом для решения систем линейных уравнений \(A\mathbf{x} = \mathbf{b}\), где \(A\) — матрица коэффициентов, \(\mathbf{x}\) — вектор неизвестных, а \(\mathbf{b}\) — вектор правых частей. Этот метод использует оптимальные параметры, основанные на полиномах Чебышева, для ускорения сходимости.

\subsection*{Основная идея метода}

Основная идея метода Ричардсона заключается в следующем итерационном процессе:
\[
\mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k (\mathbf{b} - A\mathbf{x}^{(k)}),
\]
где \(\alpha_k\) — параметр итерации.

Метод Ричардсона с Чебышевскими параметрами использует параметры \(\alpha_k\), выбранные на основе полиномов Чебышева для ускорения сходимости. Эти параметры зависят от спектрального радиуса матрицы \(A\).

\subsection*{Алгоритм метода Ричардсона с Чебышевскими параметрами}

Алгоритм метода Ричардсона с Чебышевскими параметрами включает следующие шаги:
\begin{enumerate}
    \item Задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Определить спектральные границы матрицы \(A\): \(\lambda_{\min}\) и \(\lambda_{\max}\).
    \item Рассчитать параметры Чебышева:
    \[
    \theta = \frac{\lambda_{\max} + \lambda_{\min}}{2}, \quad \delta = \frac{\lambda_{\max} - \lambda_{\min}}{2}.
    \]
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Вычислить параметр \(\alpha_k\) на основе полиномов Чебышева:
        \[
        \alpha_k = \frac{1}{\theta - \delta \cos\left(\frac{(2k+1)\pi}{2m}\right)},
        \]
        где \(m\) — общее число итераций.
        \item Обновить приближение:
        \[
        \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k (\mathbf{b} - A\mathbf{x}^{(k)}).
        \]
    \end{enumerate}
\end{enumerate}

\subsection*{Преимущества и недостатки}

Метод Ричардсона с Чебышевскими параметрами обладает следующими преимуществами и недостатками:

\begin{itemize}
    \item \textbf{Преимущества}:
    \begin{itemize}
        \item Быстрая сходимость для матриц с хорошо известными спектральными границами.
        \item Эффективность для больших и разреженных систем.
    \end{itemize}
    \item \textbf{Недостатки}:
    \begin{itemize}
        \item Необходимость предварительного определения спектральных границ матрицы \(A\).
        \item Не всегда эффективен для систем с неопределенными или изменяющимися спектральными границами.
    \end{itemize}
\end{itemize}


\section{Метод сопряжённых градиентов}
Метод сопряжённых градиентов (МСГ) является эффективным итерационным методом решения симметричных положительно определённых систем линейных уравнений. Основная идея заключается в построении последовательности сопряжённых направлений для минимизации квадратичной формы, связанной с системой уравнений.

\subsection*{Основная идея метода}

Основная идея метода заключается в минимизации квадратичного функционала:
\[
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T A \mathbf{x} - \mathbf{b}^T \mathbf{x}.
\]

Итерационный процесс основан на построении последовательности решений \(\mathbf{x}^{(k)}\) вдоль направлений \(\mathbf{p}^{(k)}\), которые являются сопряжёнными относительно матрицы \(A\).

\subsection*{Алгоритм метода сопряженных градиентов}

Алгоритм метода сопряженных градиентов включает следующие шаги:

\begin{enumerate}
    \item Задать начальное приближение \(\mathbf{x}^{(0)}\).
    \item Вычислить начальную невязку \(\mathbf{r}^{(0)} = \mathbf{b} - A\mathbf{x}^{(0)}\).
    \item Инициализировать направление \(\mathbf{p}^{(0)} = \mathbf{r}^{(0)}\).
    \item Для \(k = 0, 1, 2, \ldots\) до выполнения условия сходимости:
    \begin{enumerate}
        \item Вычислить параметр \(\alpha_k\):
        \[
        \alpha_k = \frac{\mathbf{r}^{(k)T} \mathbf{r}^{(k)}}{\mathbf{p}^{(k)T} A \mathbf{p}^{(k)}}.
        \]
        \item Обновить приближение:
        \[
        \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + \alpha_k \mathbf{p}^{(k)}.
        \]
        \item Вычислить новую невязку:
        \[
        \mathbf{r}^{(k+1)} = \mathbf{r}^{(k)} - \alpha_k A \mathbf{r}^{(k)}.
        \]
        \item Проверить условие сходимости. Если \(\|\mathbf{r}^{(k+1)}\|\) достаточно мало, завершить итерации.
        \item Вычислить параметр \(\beta_k\):
        \[
        \beta_k = \frac{\mathbf{r}^{(k+1)T} \mathbf{r}^{(k+1)}}{\mathbf{r}^{(k)T} \mathbf{r}^{(k)}}.
        \]
        \item Обновить направление:
        \[
        \mathbf{p}^{(k+1)} = \mathbf{r}^{(k+1)} + \beta_k \mathbf{p}^{(k)}.
        \]
    \end{enumerate}
\end{enumerate}

\subsection*{Преимущества и недостатки}

Метод сопряженных градиентов обладает следующими преимуществами и недостатками:

\begin{itemize}
    \item \textbf{Преимущества}:
    \begin{itemize}
        \item Быстрая сходимость для симметричных положительно определённых матриц.
        \item Эффективность по памяти, так как метод не требует хранения полной матрицы.
    \end{itemize}
    \item \textbf{Недостатки}:
    \begin{itemize}
        \item Ограничение на симметричность и положительную определённость матрицы \(A\).
        \item Чувствительность к округлостям и численным ошибкам.
    \end{itemize}
\end{itemize}

\section{Коэффициенты перекоса матрицы}
Коэффициенты перекоса матрицы характеризуют отклонение матрицы от симметричности. Для матрицы $A$ коэффициент перекоса $a_{ij}$ определяется как:
\[
a_{ij} = \frac{a_{ij} - a_{ji}}{2}.
\]
Эти коэффициенты могут использоваться для оценки симметричности матрицы и её влияния на сходимость итерационных методов.

\section*{Собственные пары матрицы}
$$Ax = \lambda x$$
Полная алгебраическая (матричная) проблема собственных значений - нахождение всех собственных пар ${\lambda, x}$ матрицы A.

Частичные проблемы - нахождение одного или нескольких собственных чисел $\lambda$ и, возможно, соотв. им векторов $x$.

Чаще всего речь идет о минимальном и максимальном по модулю собственных числах.

Знание таких характеристик матрицы позволяет,  
например, делать заключения о сходимости тех или иных итерационных  
методов, оптимизировать параметры итерационных методов, учитывать 
влияние на результаты решения алгебраических задач погрешностей  
исходных данных и вычислительных погрешностей.

$$(A - \lambda E) x = 0$$

Практическая реализация сопряжена с рядом трудностей.

\textbf{Свойство 1.} Если $\{\lambda,x\}$ - собственная пара матрицы А, а 
$\alpha\neq 0$ - некоторое число, то $\{A,\alpha х\}$ также является собственной 
парой для А. 

\textbf{Свойство 2.} Пусть $\{\mu, x\}$ - собственная пара матрицы 
А-рЕ при некотором $p\in R$. Тогда ${\lambda:=\mu + p,х}$ собственная пара 
матрицы А. 

\textbf{Свойство 3.} Если  $\{\lambda,x\}$ - собственная пара обратимой матрицы А, то $\{1/\lambda, x\}$ - собственная пара матрицы $A^{-1}$ . 

\textbf{Свойство 4.} Собственными числами диагональных и  
треугольных матриц являются их диагональные элементы. 

\section{Степенной метод}

\textbf{Определение:} Степенной метод — это итерационный метод для нахождения наибольшего по модулю собственного значения и соответствующего собственного вектора симметричной матрицы.

\textbf{Цель:} Используется для нахождения доминирующего собственного значения и собственного вектора.

\textbf{Алгоритм:}
\[
\begin{aligned}
&\textbf{Input:} \quad A \text{ (симметричная матрица)}, \mathbf{x}^{(0)} \text{ (начальное приближение)} \\
&\textbf{for } k = 0, 1, 2, \ldots \textbf{ do} \\
&\quad \mathbf{y}^{(k)} = A \mathbf{x}^{(k)} \\
&\quad \lambda^{(k)} = (\mathbf{x}^{(k)})^T \mathbf{y}^{(k)} \\
&\quad \mathbf{x}^{(k+1)} = \frac{\mathbf{y}^{(k)}}{\|\mathbf{y}^{(k)}\|} \\
&\textbf{end for}
\end{aligned}
\]

\textbf{Условие сходимости:} Метод сходится к доминирующему по модулю собственному значению матрицы \( A \), если модуль остальных собственных значений меньше доминирующего.

\section{Метод обратных итераций}

\textbf{Определение:} Метод обратных итераций — это вариант степенного метода, использующий итерации для приближённого нахождения собственного вектора, соответствующего заданному собственному значению симметричной матрицы.

\textbf{Цель:} Используется для нахождения собственного вектора, соответствующего заданному собственному значению.

\textbf{Алгоритм:}
\[
\begin{aligned}
&\textbf{Input:} \quad A \text{ (симметричная матрица)}, \sigma \text{ (приближённое собственное значение)}, \mathbf{x}^{(0)} \text{ (начальное приближение)} \\
&\textbf{for } k = 0, 1, 2, \ldots \textbf{ do} \\
&\quad \text{Решить } (A - \sigma I)\mathbf{y} = \mathbf{x}^{(k)} \\
&\quad \mathbf{x}^{(k+1)} = \frac{\mathbf{y}}{\|\mathbf{y}\|} \\
&\textbf{end for}
\end{aligned}
\]

\textbf{Условие сходимости:} Метод сходится к собственному вектору, соответствующему ближайшему к \( \sigma \) собственному значению матрицы \( A \).

\section{Метод Якоби вращений}

\textbf{Определение:} Метод Якоби вращений — это итерационный метод для нахождения всех собственных значений и собственных векторов симметричной матрицы путём последовательного вращения плоскостей.

\textbf{Цель:} Используется для полного спектрального разложения симметричной матрицы.

\textbf{Алгоритм:}
\[
\begin{aligned}
&\textbf{Input:} \quad A \text{ (симметричная матрица)}, \epsilon \text{ (порог точности)} \\
&\textbf{repeat} \\
&\quad \text{Выбрать пару индексов } (p,q) \text{ таких, что } |A_{pq}| \text{ максимально} \\
&\quad \theta = \frac{1}{2} \arctan \left( \frac{2A_{pq}}{A_{qq} - A_{pp}} \right) \\
&\quad c = \cos(\theta), \quad s = \sin(\theta) \\
&\quad J = \begin{bmatrix}
1 & & & \\
& \ddots & & \\
& & c & -s \\
& & s & c \\
& & & \ddots \\
& & & & 1
\end{bmatrix} \\
&\quad A = J^T A J \\
&\textbf{until } \max_{p \neq q} |A_{pq}| < \epsilon
\end{aligned}
\]

\textbf{Условие сходимости:} Метод сходится к спектральному разложению матрицы \( A \), т.е. к диагональной матрице с собственными значениями на диагонали.

\section{QR, LR алгоритмы}

\textbf{Определение:} QR (ортогонально-треугольное разложение) и LR (LU разложение с частичной перестановкой) алгоритмы являются методами для численного решения систем линейных уравнений и нахождения собственных значений матрицы.

\textbf{Цель:} Используются для численного решения систем линейных уравнений, нахождения собственных значений и других операций над матрицами.

\subsection{QR-разложение с использованием отражений Хаусхолдера}

\subsubsection{Определение}

QR-разложение матрицы \( A \) с использованием отражений Хаусхолдера представляет собой метод разложения \( A \) на произведение ортогональной матрицы \( Q \) и верхнетреугольной матрицы \( R \):

\[ A = QR \]

где:
\begin{itemize}
  \item \( Q \) — ортогональная матрица (\( Q^T Q = I \), \( I \) — единичная матрица),
  \item \( R \) — верхнетреугольная матрица (все элементы ниже главной диагонали равны нулю).
\end{itemize}

\subsubsection{Алгоритм}

\begin{enumerate}
  \item \textbf{Инициализация}: Пусть \( A \) — исходная матрица размера \( m \times n \), где \( m \geq n \).
  
  \item \textbf{Итерация по столбцам}:
    \begin{itemize}
      \item Для каждого столбца \( k = 1, \ldots, n-1 \):
      \item Выбирается вектор \( x \), начиная с элемента \( k \)-го до \( m \)-го столбца матрицы \( A \).
      \item Вычисляется вектор отражения Хаусхолдера \( v \), который обнуляет все элементы столбца \( x \), кроме первого.
      \item Применяется преобразование отражения Хаусхолдера к матрице \( A \), чтобы обнулить все элементы столбца \( x \), кроме первого.
    \end{itemize}
  
  \item \textbf{Формирование матрицы \( Q \)}:
    \begin{itemize}
      \item \( Q \) строится как произведение всех матриц отражений Хаусхолдера, полученных на предыдущем шаге.
    \end{itemize}
  
  \item \textbf{Формирование матрицы \( R \)}:
    \begin{itemize}
      \item \( R \) получается из последней верхнетреугольной части матрицы \( A \) после всех преобразований.
    \end{itemize}
  
  \item \textbf{Результат}:
    \begin{itemize}
      \item После завершения итераций, \( A \) представляется в виде \( A = QR \), где \( Q \) — ортогональная матрица, \( R \) — верхнетреугольная матрица.
    \end{itemize}
\end{enumerate}

\section*{Метод QR-итераций для вычисления собственных значений}

\subsection*{Описание метода}

Метод QR-итераций с использованием отражений Хаусхолдера представляет собой итерационный процесс для приближенного вычисления собственных значений матрицы \( A \).

\begin{enumerate}
  \item \textbf{Начальная матрица}: Задается начальная матрица \( A_0 = A \).
  
  \item \textbf{Итерационный процесс}:
    \begin{itemize}
      \item На \( k \)-й итерации выполняется QR-разложение матрицы \( A_{k-1} \):
      \[ A_{k-1} = Q_k R_k \]
      где \( Q_k \) — ортогональная матрица, \( R_k \) — верхнетреугольная матрица.
      
      \item Вычисляется новая матрица \( A_k \):
      \[ A_k = R_k Q_k \]
      
      Этот шаг приводит к тому, что матрица \( A_k \) приближается к верхнетреугольной форме, а её собственные значения становятся более точными.
    \end{itemize}
  
  \item \textbf{Вычисление собственных значений}:
    \begin{itemize}
      \item Собственные значения матрицы \( A \) приближенно равны элементам на диагонали матрицы \( A_k \) после сходимости итераций.
    \end{itemize}
  
  \item \textbf{Собственные векторы}:
    \begin{itemize}
      \item Для получения собственных векторов можно использовать матрицу \( Q_k \), полученную на последней итерации.
    \end{itemize}
\end{enumerate}
\subsection{LR-алгоритм}

\subsubsection{Определение}

LR-разложение матрицы \( A \) с использованием отражений Хаусхолдера представляет собой метод разложения \( A \) на произведение нижнетреугольной матрицы \( L \) и верхнетреугольной матрицы \( R \):

\[ A = LR \]

где:
\begin{itemize}
  \item \( L \) — нижнетреугольная матрица (все элементы выше главной диагонали равны нулю),
  \item \( R \) — верхнетреугольная матрица (все элементы ниже главной диагонали равны нулю).
\end{itemize}

\subsubsection{Алгоритм}

\begin{enumerate}
  \item \textbf{Инициализация}: Пусть \( A \) — исходная матрица размера \( m \times n \), где \( m \geq n \).
  
  \item \textbf{Итерация по столбцам}:
    \begin{itemize}
      \item Для каждого столбца \( k = 1, \ldots, n \):
      \item Выбирается вектор \( x \), начиная с элемента \( k \)-го до \( m \)-го столбца матрицы \( A \).
      \item Вычисляется вектор отражения Хаусхолдера \( v \), который обнуляет все элементы столбца \( x \), кроме первого.
      \item Применяется преобразование отражения Хаусхолдера к матрице \( A \), чтобы обнулить все элементы столбца \( x \), кроме первого.
    \end{itemize}
  
  \item \textbf{Формирование матриц \( L \) и \( R \)}:
    \begin{itemize}
      \item Матрица \( L \) формируется как произведение всех матриц отражений Хаусхолдера, используемых для обнуления элементов ниже диагонали.
      \item Матрица \( R \) получается из последней верхнетреугольной части матрицы \( A \) после всех преобразований.
    \end{itemize}
  
  \item \textbf{Результат}:
    \begin{itemize}
      \item После завершения итераций, \( A \) представляется в виде \( A = LR \), где \( L \) — нижнетреугольная матрица, \( R \) — верхнетреугольная матрица.
    \end{itemize}
\end{enumerate}


\textbf{Условие сходимости:} QR и LR алгоритмы гарантируют сходимость к разложению матрицы \( A \) с заданной точностью или до достижения верхнетреугольной формы (для QR) или LU формы (для LR).


Алгоритм LR аналогичен, но используется разложение $A = L R$, где $L$ нижнетреугольна, а $R$ верхнетреугольна, и обновление выполняется как $A_{k+1} = R L$.

\begin{table}[h!]
    \centering
    \begin{tabular}{| m{3cm} | m{5cm} | m{4cm} | m{4cm} |}
        \hline
        \textbf{Метод} & \textbf{Главная цель} & \textbf{Основные шаги} & \textbf{Примечания} \\ 
        \hline
        Степенной метод & Находит собственное значение с наибольшим модулем & Итерация $A \mathbf{x}$, нормализация & Прост, но медленный для близких по модулю значений \\ 
        \hline
        Метод обратных итераций & Находит собственное значение, близкое к $\mu$ & Решение $(A - \mu I) \mathbf{y} = \mathbf{x}$, нормализация & Требует $\mu$ близкое к интересующему собственному значению \\ 
        \hline
        Метод Якоби вращений & Находит все собственные значения симметричной матрицы & Обнуление внедиагональных элементов с помощью поворотов Якоби & Специфичен для симметричных матриц \\ 
        \hline
        QR алгоритм & Находит все собственные значения & Разложение $QR$, итерация $R Q$ & Быстрый и стабильный \\ 
        \hline
        LR алгоритм & Находит все собственные значения & Разложение $LR$, итерация $R L$ & Аналогичен QR, но используется другое разложение \\ 
        \hline
    \end{tabular}
    \caption{Сравнение методов нахождения собственных значений}
\end{table}
\section{Сжимающее отображение}

\textbf{Определение:} Пусть \((X, d)\) — полное метрическое пространство. Отображение \(T: X \rightarrow X\) называется \textbf{сжимающим отображением}, если существует константа \(0 \leq k < 1\), такая что для всех \(x, y \in X\) выполняется неравенство:
\[ d(T(x), T(y)) \leq k \cdot d(x, y). \]
Константа \(k\) называется \textbf{константой Липшица} отображения \(T\).

\textbf{Пример:} Рассмотрим пространство \(\mathbb{R}\) с обычной метрикой \(d(x, y) = |x - y|\) и функцию \(T(x) = \frac{1}{2}x\). Тогда для любых \(x, y \in \mathbb{R}\) имеем:
\[ d(T(x), T(y)) = \left| \frac{1}{2}x - \frac{1}{2}y \right| = \frac{1}{2}|x - y| = k \cdot d(x, y), \]
где \(k = \frac{1}{2}\). Таким образом, \(T(x) = \frac{1}{2}x\) является сжимающим отображением с константой Липшица \(k = \frac{1}{2}\).

\textbf{Теорема Банаха о неподвижной точке:} Если \(T: X \rightarrow X\) — сжимающее отображение в полном метрическом пространстве \((X, d)\), то \(T\) имеет единственную неподвижную точку \(x^*\), такую что \(T(x^*) = x^*\). Более того, для любого начального приближения \(x_0 \in X\) последовательность \(\{x_n\}\), определенная как \(x_{n+1} = T(x_n)\), сходится к \(x^*\).

\section{Производная Фреше оператора}

\textbf{Определение:} Пусть \(X\) и \(Y\) — банаховы пространства. Оператор \(F: X \rightarrow Y\) называется \textbf{дифференцируемым в точке} \(x_0 \in X\), если существует непрерывный линейный оператор \(A: X \rightarrow Y\), такой что:
\[ \lim_{h \to 0} \frac{\|F(x_0 + h) - F(x_0) - A(h)\|_Y}{\|h\|_X} = 0. \]
Оператор \(A\) называется \textbf{производной Фреше} оператора \(F\) в точке \(x_0\) и обозначается \(F'(x_0)\).

\textbf{Пример:} Рассмотрим пространство \(\mathbb{R}\) и функцию \(F(x) = x^2\). Найдем производную Фреше этой функции в точке \(x_0\). Рассмотрим оператор \(A(h) = 2x_0 h\). Тогда:
\[ F(x_0 + h) = (x_0 + h)^2 = x_0^2 + 2x_0 h + h^2. \]
Следовательно:
\[ F(x_0 + h) - F(x_0) - A(h) = x_0^2 + 2x_0 h + h^2 - x_0^2 - 2x_0 h = h^2. \]
Теперь вычислим предел:
\[ \lim_{h \to 0} \frac{|h^2|}{|h|} = \lim_{h \to 0} |h| = 0. \]
Таким образом, \(F'(x_0) = 2x_0\).

\textbf{Свойства производной Фреше:}
\begin{enumerate}
    \item \textbf{Линейность:} Если \(F\) и \(G\) — дифференцируемые операторы, и \(\alpha, \beta \in \mathbb{R}\), то \((\alpha F + \beta G)'(x) = \alpha F'(x) + \beta G'(x)\).
    \item \textbf{Производная композиции:} Если \(F: X \rightarrow Y\) и \(G: Y \rightarrow Z\) — дифференцируемые операторы, то производная композиции \((G \circ F)'(x)\) равна \(G'(F(x)) \circ F'(x)\).
    \item \textbf{Производная произведения:} Если \(F, G: X \rightarrow Y\) — дифференцируемые операторы, то \((F \cdot G)'(x) = F'(x) \cdot G(x) + F(x) \cdot G'(x)\), где произведение операторов определяется соответствующим образом.
\end{enumerate}

\section{Метод Ньютона для нелинейных задач}

Одним из популярнейших итерационных методов решения нелинейных уравнений, что связано с его идейной простотой и быстрой сходимостью, является метод Ньютона. Правило построения итерационной последовательности \( \{x_k\} \) здесь получают из геометрических соображений, откуда другое название этого метода - метод касательных, или из аналитических путем подмены данной нелинейной функции ее линейной моделью на основе формулы конечных приращений Лагранжа или формулы Тейлора, в связи с чем метод Ньютона также называют методом линеаризации. В любом случае говорить о нахождении нуля функции \( f(x) \) методом Ньютона можно лишь в предположении, что данная функция обладает достаточной гладкостью. Для простоты будем считать, что функция \( f(x) \) дважды дифференцируема на отрезке \([a; b]\), содержащем корень \( \xi \) уравнения.

Пусть \( x_k \in [a; b] \) — уже известный член последовательности приближений к \( \xi \), полученный конструируемым методом (или заданное начальное приближение \( x_0 \) при \( k=0 \)). Для любого \( x \) из \([a; b]\) можно записать формальное представление \( f(x) \) по формуле Тейлора:
\[
f(x) = f(x_k) + f'(x_k)(x - x_k) + \frac{f''(\xi_k)}{2}(x - x_k)^2, \quad (5.11)
\]
где \( \xi \) — некоторая точка между \( x \) и \( x_k \). Так как корень \( \xi \) — потенциально произвольная точка отрезка \([a; b]\), то разложение (5.11) справедливо и для \( x = \xi \), т.е. существует точка \( \xi_k \), такая что \( f(\xi_k) = 0 \), и если точка \( \xi_k \) известна, то корень \( \xi \) можно точно найти из квадратного уравнения:
\[
f(x_k) + f'(x_k)(\xi - x_k) + \frac{f''(\xi_k)}{2}(\xi - x_k)^2 = 0. \quad (5.12)
\]
Считая, что значение \( x_k \) близко к \( \xi \), т.е. разность \( (\xi - x_k) \) по модулю достаточно мала, можно рассчитывать, что величина \( (\xi - x_k)^2 \) будет тем более малой. На этом основании отбросим в (5.12) последнее слагаемое и подменим квадратное уравнение (5.12) линейным уравнением. Естественно, что при этом будет найден не корень \( \xi \), а некоторая другая точка, которую обозначим \( x_{k+1} \).

Таким образом, итерационный процесс Ньютона определяется линейным уравнением:
\[
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}, \quad (5.13)
\]
где \( k = 0, 1, 2, \ldots \), и предполагается, что хотя бы на элементах последовательности \( \{x_k\} \) первая производная данной функции не обращается в нуль.

Если в равенстве (5.13) фиксированную точку \( x_{k+1} \) заменить переменной \( x \), а 0 в правой части - переменной \( x \), то в полученном уравнении получаем:

\[
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}.
\]
\end{document}
